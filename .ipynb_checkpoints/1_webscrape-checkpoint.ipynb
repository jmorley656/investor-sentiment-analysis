{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T16:13:48.343946Z",
     "start_time": "2022-03-22T16:13:46.509578Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from time import sleep\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping script below to be updated with appropriate first and large page paramters ahead of each scrape attempt. Script will do the following:\n",
    "- iterate through each page url with the page numbers between the first and last page parameters(int)\n",
    "- sleep for a random period of 2, 3 or 4 seconds. This step may not be neccessary, but was introduced as good practise to avoid any issue that may arise from excessive page requests from a single url.\n",
    "- parse page html text\n",
    "- iterate through each of the page's 25 comments, grabbing the target html objects:\n",
    "    - date \n",
    "    - title \n",
    "    - comments \n",
    "    - content info (containing the user name, price and opion)\n",
    "- append each of the 25 comments attributes to the lists initialised at the start\n",
    "- once all pages between the first and last have been iterated through, each list will be aggregated to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T16:24:42.965792Z",
     "start_time": "2022-03-22T16:24:42.961367Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T16:22:39.464885Z",
     "start_time": "2022-03-22T16:22:08.391081Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date = []\n",
    "title = []\n",
    "comment = []\n",
    "user = []\n",
    "price = []\n",
    "opinion = []\n",
    "last_page = 10\n",
    "first_page = 1\n",
    "\n",
    "for page in list(range(first_page, last_page)):\n",
    "    sleep(randint(2,4))\n",
    "    r = requests.get(url.format(page))\n",
    "    pagesoup = BeautifulSoup(r.text, 'html.parser')\n",
    "    dates = [i.text for i in pagesoup.find_all(class_='share-chat-message__status-bar-time')]\n",
    "    titles = [i.text for i in pagesoup.find_all(class_='share-chat-message__status-bar')]\n",
    "    comments = [i.text for i in pagesoup.find_all(class_='share-chat-message__message-text')]\n",
    "    content_info = [i.text for i in pagesoup.find_all(class_='share-chat-message__content-info-box')]\n",
    "    \n",
    "    for d in dates:\n",
    "        date.append(d)\n",
    "    for t in titles:\n",
    "        title.append(t)\n",
    "    for c in comments:\n",
    "        comment.append(c)    \n",
    "    \n",
    "    for i in range(1, len(content_info), 3):\n",
    "        user.append(content_info[i-1])\n",
    "        price.append(content_info[i])\n",
    "        opinion.append(content_info[i+1])\n",
    "        \n",
    "df = pd.DataFrame({'date':date, \n",
    "                   'user':user, \n",
    "                   'title':title, \n",
    "                   'comment':comment, \n",
    "                   'opinion':opinion, \n",
    "                   'price':price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T16:22:43.908518Z",
     "start_time": "2022-03-22T16:22:43.900663Z"
    }
   },
   "outputs": [],
   "source": [
    "# incase scrape script fails, see what has bene captured\n",
    "\n",
    "print(len(date))\n",
    "print(len(title))\n",
    "print(len(comment))\n",
    "print(len(user))\n",
    "print(len(price))\n",
    "print(len(opinion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase scrape script fails, create dataframe of lists up until the script failed\n",
    "\n",
    "df = pd.DataFrame({'date':date, \n",
    "                   'user':user, \n",
    "                   'title':title, \n",
    "                   'comment':comment, \n",
    "                   'opinion':opinion, \n",
    "                   'price':price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T16:17:51.304169Z",
     "start_time": "2022-03-22T16:17:51.270517Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the date column labels the current day (Today) and 5 previous days listed as the weekday only (the day of month is omitted), some pre-cleaning must be completed on csv scrapes that include the current dat and days prior to the current day, prior to concantenating to account for different scrape dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean title column, removing the corresponding date info from the title\n",
    "df['title_clean'] = [df.loc[i,'title'].replace(df.loc[i,'date'],'') for i in range(0,len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up date column - first need to replace previous 6 days data with dates relative to scrape date\n",
    "\n",
    "Today = '22 Mar 2022'\n",
    "Mon = '21 Mar 2022'\n",
    "Sun = '20 Mar 2022'\n",
    "Sat = '19 Mar 2022'\n",
    "Fri = '18 Mar 2022'\n",
    "Thu = '17 Mar 2022'\n",
    "\n",
    "############################\n",
    "## UPDATE FOR EACH SCRAPE ##\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update date column with relative date for previous 6 days\n",
    "\n",
    "df.loc[df['date'].str.contains('Today') ,'date'] = Today\n",
    "df.loc[df['date'].str.contains('Mon') ,'date'] = Mon\n",
    "df.loc[df['date'].str.contains('Sun') ,'date'] = Sun\n",
    "df.loc[df['date'].str.contains('Sat') ,'date'] = Sat\n",
    "df.loc[df['date'].str.contains('Fri') ,'date'] = Fri\n",
    "df.loc[df['date'].str.contains('Thu') ,'date'] = Thu\n",
    "\n",
    "############################\n",
    "## UPDATE FOR EACH SCRAPE ##\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean price column\n",
    "df['price'] = df.price.str.replace('Price:  ', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "df['datetime'] = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop previous, uncleaned title column\n",
    "df.drop(columns='title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle columns to original layout\n",
    "df = df[['date', 'user', 'title_clean', 'comment', 'opinion', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename title_clean column back to title\n",
    "df.columns = ['date', 'user', 'title', 'comment', 'opinion', 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned scrape to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# df.to_csv('resources/scrape1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
